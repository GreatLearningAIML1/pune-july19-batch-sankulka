{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e7BwgvDnweso"
   },
   "source": [
    "# Project: Statistical NLP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHndGnjPwesq"
   },
   "source": [
    "Classification is probably the most popular task that you would deal with in real life.\n",
    "Text in the form of blogs, posts, articles, etc. is written every second. It is a challenge to predict the\n",
    "information about the writer without knowing about him/her.\n",
    "We are going to create a classifier that predicts multiple features of the author of a given text.\n",
    "We have designed it as a Multilabel classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset:\n",
    "\n",
    "Blog Authorship Corpus\n",
    "Over 600,000 posts from more than 19 thousand bloggers\n",
    "The Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from\n",
    "blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million\n",
    "words - or approximately 35 posts and 7250 words per person.\n",
    "Each blog is presented as a separate file, the name of which indicates a blogger id# and the\n",
    "blogger’s self-provided gender, age, industry, and astrological sign. (All are labeled for gender and\n",
    "age but for many, industry and/or sign is marked as unknown.)\n",
    "All bloggers included in the corpus fall into one of three age groups:\n",
    "8240 \"10s\" blogs (ages 13-17),\n",
    "8086 \"20s\" blogs(ages 23-27)\n",
    "2994 \"30s\" blogs (ages 33-47)\n",
    "\n",
    "For each age group, there is an equal number of male and female bloggers.\n",
    "Each blog in the corpus includes at least 200 occurrences of common English words. All formatting\n",
    "has been stripped with two exceptions. Individual posts within a single blogger are separated by the\n",
    "date of the following post and links within a post are denoted by the label urllink.\n",
    "Link to dataset:\n",
    "http://www.cs.biu.ac.il/~koppel/blogs/blogs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "00J3UcLfwess"
   },
   "source": [
    "## 1. Load the dataset (5 points)\n",
    "Tip: As the dataset is large, use fewer rows. Check what is working well on your\n",
    "machine and decide accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with zipfile.ZipFile(\"Data/blogs.zip\", 'r') as zip:\n",
    "#    zip.extractall(\"Data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = etree.XMLParser(recover=True, encoding=\"iso-8859-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "labels = []\n",
    "dir = \"Data/blogs/\"\n",
    "\n",
    "for filename in os.listdir(dir):\n",
    "    #print(filename)\n",
    "    label = filename.split(\".\")[1:5]\n",
    "    \n",
    "    parsed_xml = etree.parse(dir + filename, parser=parser)\n",
    "    for node in parsed_xml.getroot():\n",
    "        #print(node.tag)\n",
    "        if (node.tag == 'post'):\n",
    "            text.append(node.text)\n",
    "            labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(679596, 679596)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(list(zip(text, labels)), columns=['text', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n\\t \\n      Well, everyone got up and going...</td>\n",
       "      <td>[female, 37, indUnk, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\t \\n      My four-year old never stops ta...</td>\n",
       "      <td>[female, 37, indUnk, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\t \\n      Actually it's not raining yet, ...</td>\n",
       "      <td>[female, 37, indUnk, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n\\t \\n      Ha! Just set up my RSS feed - t...</td>\n",
       "      <td>[female, 37, indUnk, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n\\t \\n      Oh, which just reminded me, we ...</td>\n",
       "      <td>[female, 37, indUnk, Leo]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  \\n\\n\\t \\n      Well, everyone got up and going...   \n",
       "1  \\n\\n\\t \\n      My four-year old never stops ta...   \n",
       "2  \\n\\n\\t \\n      Actually it's not raining yet, ...   \n",
       "3  \\n\\n\\t \\n      Ha! Just set up my RSS feed - t...   \n",
       "4  \\n\\n\\t \\n      Oh, which just reminded me, we ...   \n",
       "\n",
       "                      labels  \n",
       "0  [female, 37, indUnk, Leo]  \n",
       "1  [female, 37, indUnk, Leo]  \n",
       "2  [female, 37, indUnk, Leo]  \n",
       "3  [female, 37, indUnk, Leo]  \n",
       "4  [female, 37, indUnk, Leo]  "
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(679596, 2)"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess rows of the “text” column (7.5 points)\n",
    "a. Remove unwanted characters\n",
    "b. Convert text to lowercase\n",
    "c. Remove unwanted spaces\n",
    "d. Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    new_text = str(text).lower()\n",
    "    new_text = re.sub(r\"[\\n\\t\\'+]\", \"\", new_text)\n",
    "    words = new_text.split(\" \")\n",
    "    new_text = \" \".join([word for word in words if word not in stop_words if word != \"\"])\n",
    "    #print(new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda row: preprocess_text(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. As we want to make this into a multi-label classification problem, you are required to merge all the label columns together, so that we have all the labels together for a particular sentence\n",
    "(7.5 points)\n",
    "a. Label columns to merge: “gender”, “age”, “topic”, “sign”\n",
    "b. After completing the previous step, there should be only two columns in your data\n",
    "frame i.e. “text” and “labels”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cant sleep, means stay looking wacky news item...</td>\n",
       "      <td>[female, 23, indUnk, Pisces]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>havent posted last couple days havent known wr...</td>\n",
       "      <td>[female, 25, Education, Cancer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tonight good night. driving home, felt alive c...</td>\n",
       "      <td>[female, 25, indUnk, Aries]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear president bush, one basic definitions \"so...</td>\n",
       "      <td>[male, 24, Student, Cancer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>message. university waterloo. ironically, pers...</td>\n",
       "      <td>[male, 17, indUnk, Virgo]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  cant sleep, means stay looking wacky news item...   \n",
       "1  havent posted last couple days havent known wr...   \n",
       "2  tonight good night. driving home, felt alive c...   \n",
       "3  dear president bush, one basic definitions \"so...   \n",
       "4  message. university waterloo. ironically, pers...   \n",
       "\n",
       "                            labels  \n",
       "0     [female, 23, indUnk, Pisces]  \n",
       "1  [female, 25, Education, Cancer]  \n",
       "2      [female, 25, indUnk, Aries]  \n",
       "3      [male, 24, Student, Cancer]  \n",
       "4        [male, 17, indUnk, Virgo]  "
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Separate features and labels, and split the data into training and testing (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000,), (10000,), (40000,), (10000,))"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vectorize the features (5 points)\n",
    "a. Create a Bag of Words using count vectorizer\n",
    "i. Use ngram_range=(1, 2)\n",
    "ii. Vectorize training and testing features\n",
    "b. Print the term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CountVectorizer to create document-term matrices from X_train and X_test\n",
    "vect = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2670887"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create a dictionary to get the count of every label i.e. the key will be label name and value will be the total count of the label. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_tr = mlb.fit_transform(data['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.DataFrame(labels_tr, columns=mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = dict(labels_df.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'13': 992,\n",
       " '14': 2001,\n",
       " '15': 3131,\n",
       " '16': 5296,\n",
       " '17': 5989,\n",
       " '23': 5385,\n",
       " '24': 5770,\n",
       " '25': 4897,\n",
       " '26': 3939,\n",
       " '27': 3346,\n",
       " '33': 1362,\n",
       " '34': 1644,\n",
       " '35': 1205,\n",
       " '36': 1045,\n",
       " '37': 670,\n",
       " '38': 556,\n",
       " '39': 422,\n",
       " '40': 372,\n",
       " '41': 271,\n",
       " '42': 214,\n",
       " '43': 318,\n",
       " '44': 155,\n",
       " '45': 327,\n",
       " '46': 215,\n",
       " '47': 175,\n",
       " '48': 303,\n",
       " 'Accounting': 270,\n",
       " 'Advertising': 350,\n",
       " 'Agriculture': 103,\n",
       " 'Aquarius': 3704,\n",
       " 'Architecture': 107,\n",
       " 'Aries': 4630,\n",
       " 'Arts': 2349,\n",
       " 'Automotive': 83,\n",
       " 'Banking': 292,\n",
       " 'Biotech': 173,\n",
       " 'BusinessServices': 374,\n",
       " 'Cancer': 4712,\n",
       " 'Capricorn': 3654,\n",
       " 'Chemicals': 291,\n",
       " 'Communications-Media': 1425,\n",
       " 'Construction': 81,\n",
       " 'Consulting': 439,\n",
       " 'Education': 2123,\n",
       " 'Engineering': 854,\n",
       " 'Environment': 37,\n",
       " 'Fashion': 363,\n",
       " 'Gemini': 3922,\n",
       " 'Government': 481,\n",
       " 'HumanResources': 203,\n",
       " 'Internet': 1231,\n",
       " 'InvestmentBanking': 91,\n",
       " 'Law': 666,\n",
       " 'LawEnforcement-Security': 134,\n",
       " 'Leo': 3883,\n",
       " 'Libra': 4480,\n",
       " 'Manufacturing': 173,\n",
       " 'Maritime': 22,\n",
       " 'Marketing': 357,\n",
       " 'Military': 202,\n",
       " 'Museums-Libraries': 225,\n",
       " 'Non-Profit': 1072,\n",
       " 'Pisces': 4009,\n",
       " 'Publishing': 586,\n",
       " 'RealEstate': 192,\n",
       " 'Religion': 368,\n",
       " 'Sagittarius': 3697,\n",
       " 'Science': 475,\n",
       " 'Scorpio': 4228,\n",
       " 'Sports-Recreation': 221,\n",
       " 'Student': 11364,\n",
       " 'Taurus': 4629,\n",
       " 'Technology': 3187,\n",
       " 'Telecommunications': 266,\n",
       " 'Tourism': 137,\n",
       " 'Transportation': 150,\n",
       " 'Virgo': 4452,\n",
       " 'female': 24737,\n",
       " 'indUnk': 18483,\n",
       " 'male': 25263}"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Transform the labels - (7.5 points)\n",
    "As we have noticed before, in this task each example can have multiple tags. To deal with\n",
    "such kind of prediction, we need to transform labels in a binary form and the prediction will be\n",
    "a mask of 0s and 1s. For this purpose, it is convenient to use MultiLabelBinarizer from sklearn\n",
    "a. Convert your train and test labels using MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tr = mlb.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_tr = mlb.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Choose a classifier - (5 points)\n",
    "In this task, we suggest using the One-vs-Rest approach, which is implemented in\n",
    "OneVsRestClassifier class. In this approach k classifiers (= number of tags) are trained. As a\n",
    "basic classifier, use LogisticRegression . It is one of the simplest methods, but often it\n",
    "performs good enough in text classification tasks. It might take some time because the\n",
    "number of classifiers to train is large.\n",
    "a. Use a linear classifier of your choice, wrap it up in OneVsRestClassifier to train it on\n",
    "every label\n",
    "b. As One-vs-Rest approach might not have been discussed in the sessions, we are\n",
    "providing you the code for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression(solver='lbfgs', multi_class=\"ovr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovrc = OneVsRestClassifier(lgr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Fit the classifier, make predictions and get the accuracy (5 points)\n",
    "a. Print the following\n",
    "i. Accuracy score\n",
    "ii. F1 score\n",
    "iii. Average precision score\n",
    "iv. Average recall score\n",
    "v. Tip: Make sure you are familiar with all of them. How would you expect the\n",
    "things to work for the multi-label scenario? Read about micro/macro/weighted\n",
    "averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='ovr', n_jobs=None,\n",
       "                                                 penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovrc.fit(X_train_dtm, y_train_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9022\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy: \", metrics.accuracy_score(y_train_tr, ovrc.predict(X_train_dtm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ovrc.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.0108\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy: \", metrics.accuracy_score(y_test_tr, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The accuracy measurement of multi-label classification is different than single-label classification. In multi-lable classification, mis-classification is no hard right or wrong. The subset of prediction class is better than non-predicting even a single label.\n",
    "\n",
    "- In micro-averaging all TPs, TNs, FPs and FNs for each class are summed up and then the average is taken. Micro-average aggregates the contributions of all classes to compute the average metric.\n",
    "\n",
    "- Micro-averaging can be a useful measure when the class imbalance is already known.\n",
    "\n",
    "- Macro-average computes the metric independently for each class and then take the average i.e. treating all classes equally.\n",
    "\n",
    "- Macro-averaging is useful when we want to know how the system performs overall across the sets of data.\n",
    "\n",
    "- In Weighted-averaging, each class contribution to the average is weighted by the relative number of examples available for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaging F1 Score:  0.3212034894071569\n",
      "Macro-averaging F1 Score:  0.0707366509287803\n",
      "Weighted-averaging F1 Score:  0.2541110811878727\n"
     ]
    }
   ],
   "source": [
    "print(\"Micro-averaging F1 Score: \", metrics.f1_score(y_test_tr, y_pred, average='micro'))\n",
    "print(\"Macro-averaging F1 Score: \", metrics.f1_score(y_test_tr, y_pred, average='macro'))\n",
    "print(\"Weighted-averaging F1 Score: \", metrics.f1_score(y_test_tr, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaging Precision Score:  0.5578849721706864\n",
      "Macro-averaging Precision Score:  0.25029888833293557\n",
      "Weighted-averaging Precision Score:  0.4153331348633759\n"
     ]
    }
   ],
   "source": [
    "print(\"Micro-averaging Precision Score: \", metrics.precision_score(y_test_tr, y_pred, average='micro'))\n",
    "print(\"Macro-averaging Precision Score: \", metrics.precision_score(y_test_tr, y_pred, average='macro'))\n",
    "print(\"Weighted-averaging Precision Score: \", metrics.precision_score(y_test_tr, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaging Recall Score:  0.225525\n",
      "Macro-averaging Recall Score:  0.049242089378424045\n",
      "Weighted-averaging Recall Score:  0.225525\n"
     ]
    }
   ],
   "source": [
    "print(\"Micro-averaging Recall Score: \", metrics.recall_score(y_test_tr, y_pred, average='micro'))\n",
    "print(\"Macro-averaging Recall Score: \", metrics.recall_score(y_test_tr, y_pred, average='macro'))\n",
    "print(\"Weighted-averaging Recall Score: \", metrics.recall_score(y_test_tr, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Print true label and predicted label for any five examples (7.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('17', 'Libra', 'female', 'indUnk')\n",
      "('Student', 'female')\n",
      "('27', 'Leo', 'female', 'indUnk')\n",
      "('Taurus', 'male')\n",
      "('33', 'Biotech', 'Capricorn', 'female')\n",
      "('female',)\n",
      "('34', 'Aquarius', 'Education', 'male')\n",
      "('34', 'Aquarius', 'Education', 'male')\n",
      "('14', 'Capricorn', 'indUnk', 'male')\n",
      "('male',)\n"
     ]
    }
   ],
   "source": [
    "for ii in np.random.randint(1, len(y_test_tr), 5):\n",
    "    print(mlb.inverse_transform(y_test_tr)[ii])\n",
    "    print(mlb.inverse_transform(y_pred)[ii])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have used only 50000 data points and it took almost 3 hours of training hours to reach training accuracy of 90%.\n",
    "\n",
    "- The testing accuracy of the model is very poor, just close to 1%. Some more iterations and model tuning exercise should be conducted to improve it.\n",
    "\n",
    "- This is also reflected from the above True and Predicted labels. We couldn't get prediction for all classes and these classes are also not predicted correctly.\n",
    "\n",
    "- We also see significant difference between the Precision/Recall Score by Micro/Macro averaging methods. The label classes seem to be highly imbalance, ranges from 1 to 5000 data points. So, it would be better to go with Micro-averaging method."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "textClassificationWithML_Advanced.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
